<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Properties of Frank-Wolfe's Method and LMO | Tao Hu</title>
  <link href="https://fonts.googleapis.com/css2?family=Lora:wght@400;600&family=Open+Sans:wght@300;400;600;700&display=swap" rel="stylesheet">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">
  <link rel="stylesheet" href="../css/style.css">
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body class="blog-page">
  <nav class="navbar navbar-expand-lg navbar-dark fixed-top">
    <div class="container">
      <a class="navbar-brand" href="../index.html">Tao Hu</a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarNav">
        <ul class="navbar-nav ms-auto">
          <li class="nav-item"><a class="nav-link" href="../index.html#about">About</a></li>
          <li class="nav-item"><a class="nav-link" href="../index.html#education">Education</a></li>
          <li class="nav-item"><a class="nav-link" href="../index.html#research">Research</a></li>
          <li class="nav-item"><a class="nav-link" href="../index.html#experience">Experience</a></li>
          <li class="nav-item"><a class="nav-link" href="../index.html#honors">Honors</a></li>
          <li class="nav-item"><a class="nav-link" href="../index.html#skills">Skills</a></li>
          <li class="nav-item"><a class="nav-link" href="../index.html#blog">Blog</a></li>
          <li class="nav-item"><a class="nav-link" href="../index.html#contact">Contact</a></li>
        </ul>
      </div>
    </div>
  </nav>

  <header class="blog-hero">
    <div class="overlay"></div>
    <div class="container text-center text-white">
      <h1 class="display-5">Properties of Frank-Wolfe's Method and Linear Minimization Oracle</h1>
      <p class="lead">IEOR 262B final project (UC Berkeley visiting student) exploring robustness of Frank-Wolfe and the cost of LMOs versus projections.</p>
      <div class="blog-meta">
        <span><i class="bi bi-calendar-event"></i> March 2025</span>
        <span><i class="bi bi-tag"></i> Optimization</span>
      </div>
    </div>
  </header>

  <main class="blog-content py-5">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-xl-9">
          <article class="blog-post">
            <section class="mb-5">
              <h2>Abstract</h2>
              <p class="mb-3">This note records my IEOR 262B final project as a visiting student at UC Berkeley. I examine the robustness of the Frank-Wolfe method under inexact or stochastic gradients and compare projection operators with the linear minimization oracle.</p>
              <p class="mb-0"><strong>Keywords:</strong> Frank-Wolfe, conditional gradients, inexact oracle, stochastic optimization, heavy-tailed noise, projection vs.&nbsp;LMO.</p>
            </section>

            <section class="mb-5">
              <h2 id="introduction">1.&nbsp;Introduction and related work</h2>
              <p><strong>Background.</strong> Let \(Q\subset\mathbb{R}^d\) be a compact convex set and \(h:Q\to\mathbb{R}\) be the objective function. Our goal is to solve the maximization problem</p>
              <p class="math-display">\[\max_{\lambda\in Q} h(\lambda).\]</p>
              <p>The Frank-Wolfe method computes at \(\lambda_k\in Q\) the linearization point</p>
              <p class="math-display">\[\tilde{\lambda}_k\in \arg\max_{\lambda\in Q}\Big\{h(\lambda_k)+\nabla h(\lambda_k)^\top(\lambda-\lambda_k)\Big\}\]</p>
              <p>and updates \(\lambda_{k+1}=(1-\bar{\alpha}_k)\lambda_k+\bar{\alpha}_k \tilde{\lambda}_k\) with \(\bar{\alpha}_k\in[0,1)\). When \(\nabla h\) is \(L\)-Lipschitz on \(Q\) and \(Q\) has diameter \(D\), the method achieves the classical \(\mathcal{O}(LD^2/k)\) rate (Jaggi, 2013; Freund and Grigas, 2013). Auxiliary sequences often appear in the analysis:</p>
              <p class="math-display">\[\beta_k = \frac{1}{\prod_{j=1}^{k-1} (1-\bar{\alpha}_j)},\qquad \alpha_k = \frac{\beta_k \bar{\alpha}_k}{1- \bar{\alpha}_k}, \qquad k \ge 1,\]</p>
              <p>where we follow the conventions \(\prod_{j=1}^0 (\cdot) = 1\) and \(\sum_{i=1}^0 (\cdot) = 0\).</p>
              <div class="algorithm mb-4">
                <div class="algorithm-header">Algorithm 1 &mdash; Frank-Wolfe method for maximizing \(h(\lambda)\)</div>
                <ol class="algorithm-steps">
                  <li>Pick initial point \(\lambda_0 \in Q\), initial upper bound \(U_{\mathrm{init}}\) (usually determined by prior knowledge, if no prior knowledge is assumed, set \(U_{\mathrm{init}} = +\infty\)), and take \(k \leftarrow 0\).</li>
                  <li>Evaluate \(\nabla h(\lambda_k)\).</li>
                  <li>Compute \(\tilde{\lambda}_k = \arg\max_{\lambda \in Q}\{h(\lambda_k) + \nabla h(\lambda_k)^\top(\lambda - \lambda_k)\}\).</li>
                  <li>Set \(U_k^w = h(\lambda_k) + \nabla h(\lambda_k)^\top(\tilde{\lambda}_k - \lambda_k)\) and \(G_k = \nabla h(\lambda_k)^\top(\tilde{\lambda}_k - \lambda_k)\).</li>
                  <li>Update \(U_k = \min\{U_{k - 1}, U_k^{w}\}\) if \(k \geq 1\) and \(U_0 = \min\{U_{\mathrm{init}}, U_0^{w}\}\) if \(k = 0\).</li>
                  <li>Set \(\lambda_{k+1} = \lambda_k + \bar{\alpha}_k(\tilde{\lambda}_k - \lambda_k)\) with \(\bar{\alpha}_0 = 1\) and \(\bar{\alpha}_k \in [0,1)\) for all \(k \ge 1\).</li>
                </ol>
              </div>
              <p><strong>Inexact information.</strong> A central question is the robustness of Frank-Wolfe under inexact gradients. With unbiased gradients and bounded variance (or sub-Gaussian tails), stochastic Frank-Wolfe variants achieve \(\mathcal{O}(1/\sqrt{k})\) rates; variance reduction yields faster rates under additional structure (Hazan and Luo, 2016; Reddi et&nbsp;al., 2016; Goldfarb et&nbsp;al., 2017; Lu and Freund, 2018; Locatello et&nbsp;al., 2019; Zhang et&nbsp;al., 2020). For heavy-tailed noise, robust stochastic Frank-Wolfe with clipping or robust estimation attains high-probability guarantees (Tang et&nbsp;al., 2022; Sfyraki and Wang, 2025). For deterministic noise bounded by \(\delta\) the classical analysis of Freund and Grigas (2013) applies. We also consider objective functions that are convex but non-smooth and can be accessed through a \((\delta, L)\)-oracle.</p>
              <p>Two oracle models are of interest. A <em>\(\delta\)-oracle</em> returns \(g_{\delta}(\bar{\lambda})\) such that, for all \(\lambda,\bar{\lambda}\in Q\),</p>
              <p class="math-display">\[\big|\big(\nabla h(\bar{\lambda})-g_{\delta}(\bar{\lambda})\big)^\top(\lambda-\bar{\lambda})\big| \le \delta.\]</p>
              <p>The second is the <em>\((\delta,L)\)-oracle</em> of Devolder, Glineur, and Nesterov (2014), which returns \((h_{\delta,L}(\bar{\lambda}), g_{\delta,L}(\bar{\lambda}))\) satisfying first-order upper and lower models with curvature \(L\) and additive error \(\delta\). This oracle permits accumulation of errors and will be analyzed in Section&nbsp;4.</p>
              <p><strong>LMO vs.&nbsp;projection.</strong> The linear minimization oracle is often easier to compute than projections. Set-specific comparisons appear in Combettes and Pokutta (2021) and Pokutta (2024), while Woodstock (2025) showed that exact projection is never easier than obtaining an \(\varepsilon\)-accurate LMO. We extend this observation to approximate projections: a single \(K\)-projection at a scaled point yields an \(\varepsilon\)-accurate LMO.</p>
              <p><strong>Our contributions.</strong></p>
              <ol class="mb-0">
                <li><em>Frank-Wolfe with a \(\delta\)-oracle (nonconvex).</em> For \(L\)-smooth nonconvex \(f\) over a compact convex set we show that Frank-Wolfe with a directional \(\delta\)-oracle achieves
                  <p class="math-display">\[\min_{0\le k\le K} g(x^k) \le \sqrt{\frac{2C\,\big(f(x^0)-f_{\inf}\big)}{K+1}} + 4\delta,\]</p>
                  <p class="mb-2">which is the standard \(\mathcal{O}(1/\sqrt{K})\) rate to an \(\mathcal{O}(\delta)\) neighborhood in the Frank-Wolfe gap.</p>
                </li>
                <li><em>Frank-Wolfe with a \((\delta,L)\)-oracle.</em> We prove that Frank-Wolfe attains a Frank-Wolfe gap of \(\mathcal{O}(\sqrt{\delta})\) and provide a counterexample showing that this order cannot be improved.</li>
                <li><em>Projection vs.&nbsp;LMO.</em> We show that a \(K\)-approximate projection at \(-\lambda x\) produces an \(\varepsilon\)-accurate LMO at \(x\) with \(\varepsilon = \mathcal{O}((K + \delta_C^2)/\lambda)\), so coarse projections are not cheaper than accurate LMOs.</li>
              </ol>
            </section>

            <section class="mb-5">
              <h2 id="delta-oracle">2.&nbsp;Frank-Wolfe with a \(\delta\)-oracle: main result and a tight example</h2>
              <p>Assume \(Q\subset\mathbb{R}^d\) is compact and convex with diameter \(D\), and \(h:Q\to\mathbb{R}\) is concave with \(L\)-Lipschitz gradient. We run Frank-Wolfe using the \(\delta\)-oracle in the linearization.</p>
              <div class="algorithm mb-4">
                <div class="algorithm-header">Algorithm 2 &mdash; Frank-Wolfe with a gradient \(\delta\)-oracle (maximization)</div>
                <ol class="algorithm-steps">
                  <li>Initialize \(\lambda_0\in Q\).</li>
                  <li>For \(k = 0,1,2,\ldots\):</li>
                  <li class="ms-4">Query \(g_{\delta}(\lambda_k)\).</li>
                  <li class="ms-4">Compute \(\tilde{\lambda}_k\in\arg\max_{\lambda\in Q}\big\{h(\lambda_k)+g_{\delta}(\lambda_k)^\top(\lambda-\lambda_k)\big\}\).</li>
                  <li class="ms-4">Update \(\lambda_{k+1}=\lambda_k+\bar{\alpha}_k(\tilde{\lambda}_k-\lambda_k)\) with \(\bar{\alpha}_k\in[0,1)\).</li>
                </ol>
              </div>
              <p>We first transfer the Wolfe upper bound under a \(\delta\)-oracle.</p>
              <div class="theorem-block">
                <h4>Lemma&nbsp;1 (Wolfe bound transfer under a \(\delta\)-oracle)</h4>
                <p>For any \(\lambda_k\in Q\),</p>
                <p class="math-display">\[h^* \le h(\lambda_k) + \max_{\lambda\in Q} g_{\delta}(\lambda_k)^\top(\lambda-\lambda_k) + \delta.\]</p>
                <p><strong>Proof.</strong> By smoothness, \(h(\lambda) \le h(\lambda_k) + \nabla h(\lambda_k)^\top(\lambda-\lambda_k)\). The oracle inequality ensures \(\nabla h(\lambda_k)^\top(\lambda-\lambda_k) \le g_{\delta}(\lambda_k)^\top(\lambda-\lambda_k) + \delta\). Maximizing over \(\lambda\in Q\) yields the claim. \(\square\)</p>
              </div>
              <p>We also recall a subproblem accuracy transfer.</p>
              <div class="theorem-block">
                <h4>Proposition&nbsp;1 (Freund &amp; Grigas, 2013)</h4>
                <p>If \(\tilde{\lambda}\in\arg\max_{\lambda\in Q} g_{\delta}(\bar{\lambda})^\top\lambda\), then</p>
                <p class="math-display">\[\nabla h(\bar{\lambda})^\top\tilde{\lambda} \ge \max_{\lambda\in Q} \nabla h(\bar{\lambda})^\top\lambda - 2\delta.\]</p>
              </div>
              <div class="theorem-block">
                <h4>Theorem&nbsp;2 (Nonaccumulation under a \(\delta\)-oracle)</h4>
                <p>Let \(Q\) be compact convex with diameter \(D\), and let \(h\) be concave with \(L\)-Lipschitz gradient on \(Q\). Suppose \(g_{\delta}\) satisfies the \(\delta\)-oracle condition. For the iterates of Algorithm&nbsp;2 with open-loop step sizes satisfying \(\sum_k \bar{\alpha}_k = \infty\) and \(\bar{\alpha}_k \downarrow 0\),</p>
                <p class="math-display">\[h^* - h(\lambda_{k+1}) \le (1-\bar{\alpha}_k)\big(h^* - h(\lambda_k)\big) + 2\bar{\alpha}_k\delta + \tfrac{1}{2} LD^2 \bar{\alpha}_k^2,\]</p>
                <p>and consequently \(\limsup_{k\to\infty} (h^* - h(\lambda_k)) \le 2\delta\).</p>
                <p><strong>Proof.</strong> From Lipschitz smoothness and the oracle condition, we obtain</p>
                <p class="math-display">\begin{aligned}
                  h(\lambda_{k+1}) &\ge h(\lambda_k) + g_{\delta}(\lambda_k)^\top (\lambda_{k+1} - \lambda_k) - \delta - \tfrac{1}{2} L \|\lambda_{k+1} - \lambda_k\|^2 \\
                  &= h(\lambda_k) + \bar{\alpha}_k g_{\delta}(\lambda_k)^\top (\tilde{\lambda}_k - \lambda_k) - \delta - \tfrac{1}{2} L \bar{\alpha}_k^2 \|\tilde{\lambda}_k - \lambda_k\|^2 \\
                  &\ge (1-\bar{\alpha}_k) h(\lambda_k) + \bar{\alpha}_k\big(h(\lambda_k) + g_{\delta}(\lambda_k)^\top(\tilde{\lambda}_k - \lambda_k) + \delta\big) \\
                  &\quad - 2\bar{\alpha}_k \delta - \tfrac{1}{2} L D^2 \bar{\alpha}_k^2 \\
                  &\ge (1-\bar{\alpha}_k) h(\lambda_k) + \bar{\alpha}_k h^* - 2\bar{\alpha}_k\delta - \tfrac{1}{2} L D^2 \bar{\alpha}_k^2,
                \end{aligned}</p>
                <p>where we used \(\|\tilde{\lambda}_k - \lambda_k\| \le D\) and Lemma&nbsp;1 in the final step. Subtracting from \(h^*\) yields the inequality. The stated limit follows by unrolling the recurrence. \(\square\)</p>
              </div>
              <div class="theorem-block">
                <h4>Example&nbsp;1 (Tightness up to constants)</h4>
                <p>Let \(Q = [-1,1]\) and \(h(\lambda) = -\tfrac{1}{2} (\lambda - 1)^2\), so \(L = 1\) and \(D = 2\). Define a \(\delta\)-oracle by \(g_{\delta}(\lambda) = \nabla h(\lambda) + \tfrac{\delta}{D} \operatorname{sign}(\lambda)\). Frank-Wolfe with \(\bar{\alpha}_k = 2/(k+2)\) converges to a neighborhood whose size is proportional to \(\delta\).</p>
              </div>
            </section>

            <section class="mb-5">
              <h2 id="nonconvex">3.&nbsp;Nonconvex objectives with a directional \(\delta\)-oracle</h2>
              <p>We now consider nonconvex minimization over a compact convex set \(S\subset\mathbb{R}^d\): \(\min_{x\in S} f(x)\). Assume \(f\) is differentiable with \(L\)-Lipschitz gradient on \(S\). Let \(D = \operatorname{Diam}(S)\) and define \(G = \sup_{x\in S}\|\nabla f(x)\|\) and \(C = \max\{LD^2, GD\}\). The Frank-Wolfe gap at \(x\) is</p>
              <p class="math-display">\[g(x) = \max_{s\in S} \langle \nabla f(x), x - s \rangle.\]</p>
              <p>We assume access to a <em>directional \(\delta\)-oracle</em>, i.e., for every \(x\in S\) there exists \(g_{\delta}(x)\) such that</p>
              <p class="math-display">\[\big|\langle \nabla f(x) - g_{\delta}(x), s - x \rangle\big| \le \delta, \qquad \forall s \in S.\]</p>
              <p>Define the approximate Frank-Wolfe gap \(\tilde{g}(x) = \max_{s\in S} \langle g_{\delta}(x), x - s \rangle\). Then</p>
              <p class="math-display">\[|g(x) - \tilde{g}(x)| \le \delta, \qquad \langle \nabla f(x), x - s_{\delta}(x) \rangle \ge \tilde{g}(x) - \delta,\]</p>
              <p>where \(s_{\delta}(x)\in\arg\max_{s\in S} \langle g_{\delta}(x), x - s \rangle\).</p>
              <div class="algorithm mb-4">
                <div class="algorithm-header">Algorithm 3 &mdash; Nonconvex Frank-Wolfe with a directional \(\delta\)-oracle</div>
                <ol class="algorithm-steps">
                  <li><strong>Input:</strong> \(x^0\in S\), curvature constant \(C \ge \max\{LD^2, GD\}\), error level \(\delta \ge 0\).</li>
                  <li>For \(k = 0,1,2,\ldots\):</li>
                  <li class="ms-4">Obtain \(g_{\delta}(x^k)\) satisfying the directional inequality and set \(s^k\in\arg\max_{s\in S} \langle g_{\delta}(x^k), x^k - s \rangle\); let \(\tilde{g}_k = \langle g_{\delta}(x^k), x^k - s^k \rangle\).</li>
                  <li class="ms-4">Choose the step size \(\bar{\alpha}_k = \min\{(\tilde{g}_k - 2\delta)_+/C, 1\}\), where \((u)_+ = \max\{u, 0\}\).</li>
                  <li class="ms-4">Update \(x^{k+1} = x^k + \bar{\alpha}_k (s^k - x^k)\).</li>
                </ol>
              </div>
              <div class="theorem-block">
                <h4>Lemma&nbsp;2 (One-step decrease)</h4>
                <p>The iterates of Algorithm&nbsp;3 satisfy</p>
                <p class="math-display">\[f(x^{k+1}) \le f(x^k) - \frac{(\tilde{g}_k - 2\delta)_+^2}{2C}.\]</p>
                <p><strong>Proof.</strong> Lipschitz smoothness yields</p>
                <p class="math-display">\begin{aligned}
                  f(x^{k+1}) &\le f(x^k) + \bar{\alpha}_k \langle \nabla f(x^k), s^k - x^k \rangle + \tfrac{L}{2} \bar{\alpha}_k^2 \|s^k - x^k\|^2 \\
                  &\le f(x^k) - \bar{\alpha}_k (g(x^k) - \delta) + \tfrac{C}{2} \bar{\alpha}_k^2,
                \end{aligned}</p>
                <p>where we used \(\|s^k - x^k\| \le D\) and the directional oracle. Since \(g(x^k) \ge \tilde{g}_k - \delta\), we have</p>
                <p class="math-display">\[f(x^{k+1}) \le f(x^k) - \bar{\alpha}_k (\tilde{g}_k - 2\delta) + \tfrac{C}{2} \bar{\alpha}_k^2.\]</p>
                <p>With \(\bar{\alpha}_k = (\tilde{g}_k - 2\delta)_+/C\), the result follows because \(\bar{\alpha}_k = 0\) whenever \(\tilde{g}_k - 2\delta \le 0\). \(\square\)</p>
              </div>
              <div class="theorem-block">
                <h4>Theorem&nbsp;3 (Directional \(\delta\)-oracle for nonconvex objectives)</h4>
                <p>Let \(f\) be \(L\)-smooth on a compact convex set \(S\) of diameter \(D\) and choose \(C \ge \max\{LD^2, GD\}\). Suppose the directional \(\delta\)-oracle is available. Then for all \(K \ge 0\), the iterates of Algorithm&nbsp;3 obey</p>
                <p class="math-display">\[\min_{0\le k\le K} g(x^k) \le \sqrt{\frac{2C\,\big(f(x^0) - f_{\inf}\big)}{K+1}} + 3\delta,\]</p>
                <p>where \(f_{\inf} = \inf_{x\in S} f(x)\). Consequently, to reach a Frank-Wolfe gap at most \(\varepsilon > 4\delta\) it suffices to take \(K + 1 \ge 2C\big(f(x^0) - f_{\inf}\big)/(\varepsilon - 3\delta)^2\).</p>
                <p><strong>Proof.</strong> Summing Lemma&nbsp;2 from \(k = 0\) to \(K\) gives</p>
                <p class="math-display">\[\sum_{k=0}^K (\tilde{g}_k - 2\delta)_+^2 \le 2C\big(f(x^0) - f(x^{K+1})\big) \le 2C\big(f(x^0) - f_{\inf}\big).\]</p>
                <p>Hence \(\min_{0\le k\le K} (\tilde{g}_k - 2\delta)_+ \le \sqrt{2C\big(f(x^0) - f_{\inf}\big)/(K+1)}\). Using \(g(x^k) \le \tilde{g}_k + \delta\) yields the desired bound. \(\square\)</p>
              </div>
              <p><strong>Remarks.</strong> When \(\delta = 0\) the classical nonconvex rate is recovered. For \(\delta > 0\), the method converges to an \(\mathcal{O}(\delta)\) neighborhood in the Frank-Wolfe gap. The stepsize uses \(\tilde{g}_k\), obtained while solving the LMO with \(g_{\delta}\). Any \(C \ge LD^2\) works in Lemma&nbsp;2; choosing \(C \ge \max\{LD^2, GD\}\) ensures \(\bar{\alpha}_k \le 1\) without additional truncation.</p>
            </section>

            <section class="mb-5">
              <h2 id="delta-L">4.&nbsp;Frank-Wolfe with a \((\delta,L)\)-oracle: accumulation made explicit</h2>
              <p>We adopt the Devolder&ndash;Glineur&ndash;Nesterov \((\delta,L)\)-oracle for the concave function \(h\): for any \(\bar{\lambda}\in Q\) and \(\lambda\in Q\) the oracle returns \((h_{\delta,L}(\bar{\lambda}), g_{\delta,L}(\bar{\lambda}))\) obeying</p>
              <p class="math-display">\begin{aligned}
                h(\lambda) &\le h_{\delta,L}(\bar{\lambda}) + \langle g_{\delta,L}(\bar{\lambda}), \lambda - \bar{\lambda} \rangle + \tfrac{L}{2} \|\lambda - \bar{\lambda}\|^2 + \delta, \\
                h(\lambda) &\ge h_{\delta,L}(\bar{\lambda}) + \langle g_{\delta,L}(\bar{\lambda}), \lambda - \bar{\lambda} \rangle - \delta.
              \end{aligned}</p>
              <div class="algorithm mb-4">
                <div class="algorithm-header">Algorithm 4 &mdash; Frank-Wolfe with a \((\delta,L)\)-oracle (maximization)</div>
                <ol class="algorithm-steps">
                  <li>Initialize \(\lambda_0\in Q\) and \(U_0 = 0\).</li>
                  <li>For \(k = 0,1,2,\ldots\):</li>
                  <li class="ms-4">Query \((h_{\delta,L}(\lambda_k), g_{\delta,L}(\lambda_k))\).</li>
                  <li class="ms-4">Compute \(\tilde{\lambda}_k\in\arg\max_{\lambda\in Q} \langle g_{\delta,L}(\lambda_k), \lambda - \lambda_k \rangle\) and set \(U_k^w = h(\lambda_k) + \nabla h(\lambda_k)^\top (\tilde{\lambda}_k - \lambda_k)\).</li>
                  <li class="ms-4">Update \(\lambda_{k+1} = \lambda_k + \bar{\alpha}_k (\tilde{\lambda}_k - \lambda_k)\) with \(\bar{\alpha}_k\in[0,1)\) and set \(U_{k+1} = \min\{U_k, U_k^w\}\).</li>
                </ol>
              </div>
              <div class="theorem-block">
                <h4>Theorem&nbsp;4 (Complexity with a \((\delta,L)\)-oracle)</h4>
                <p>If Algorithm&nbsp;4 uses the step-size sequence \(\{\bar{\alpha}_k\}\), then for all \(k \ge 0\)</p>
                <p class="math-display">\[U_{k+1} - h(\lambda_{k+1}) \le \frac{U_1 - h(\lambda_1)}{\beta_{k+1}} + \frac{\tfrac{1}{2} C \sum_{i=1}^k \alpha_i^2 / \beta_{i+1}}{\beta_{k+1}} + \frac{2\delta \sum_{i=1}^k \beta_{i+1}}{\beta_{k+1}},\]</p>
                <p>where \(C = L D^2\).</p>
                <p><strong>Proof.</strong> From the oracle relations we obtain</p>
                <p class="math-display">\begin{aligned}
                  h(\lambda_{k+1}) &\ge h(\lambda_k) + g_{\delta,L}(\lambda_k)^\top (\lambda_{k+1} - \lambda_k) - \delta - \tfrac{1}{2} L \|\lambda_{k+1} - \lambda_k\|^2 \\
                  &= h(\lambda_k) + \bar{\alpha}_k g_{\delta,L}(\lambda_k)^\top (\tilde{\lambda}_k - \lambda_k) - \delta - \tfrac{1}{2} L \bar{\alpha}_k^2 \|\tilde{\lambda}_k - \lambda_k\|^2 \\
                  &\ge (1 - \bar{\alpha}_k) h(\lambda_k) + \bar{\alpha}_k \big(h(\lambda_k) + g_{\delta,L}(\lambda_k)^\top (\tilde{\lambda}_k - \lambda_k) + \delta\big) \\
                  &\quad - (1 + \bar{\alpha}_k) \delta - \tfrac{1}{2} C \bar{\alpha}_k^2 \\
                  &\ge (1 - \bar{\alpha}_k) h(\lambda_k) + \bar{\alpha}_k U_k - 2\delta - \tfrac{1}{2} C \bar{\alpha}_k^2.
                \end{aligned}</p>
                <p>Hence</p>
                <p class="math-display">\[U_k - h(\lambda_{k+1}) \le (1 - \bar{\alpha}_k)(U_{k-1} - h(\lambda_k)) + 2\delta + \tfrac{1}{2} C \bar{\alpha}_k^2.\]</p>
                <p>Multiplying by \(\beta_k\) and summing gives</p>
                <p class="math-display">\[\beta_k (U_k - h(\lambda_{k+1})) \le (U_0 - h(\lambda_1)) + 2\delta \sum_{j=1}^k \beta_j + \tfrac{1}{2} C \sum_{j=1}^k \bar{\alpha}_j^2 \beta_j.\]</p>
                <p>Dividing by \(\beta_k\) yields the stated inequality. \(\square\)</p>
              </div>
              <p class="mb-3">Error accumulation implies that achieving a first-order accurate solution requires a second-order accurate gradient. Indeed, the right-hand side can be written as</p>
              <p class="math-display">\[\frac{U_0 - h(\lambda_1)}{\beta_{k+1}} + \frac{\sum_{i=1}^k \left(\tfrac{1}{2} C \alpha_i^2 / \beta_{i+1} + \beta_{i+1} \delta\right)}{\beta_{k+1}} \ge \frac{U_0 - h(\lambda_1)}{\beta_{k+1}} + \sqrt{2 C \delta}\, \frac{\beta_{k+1} - 1}{\beta_{k+1}}.\]</p>
              <p>For large \(k\) we have \(0 < (\beta_2 - 1)/\beta_2 \le (\beta_{k+1} - 1)/\beta_{k+1} \le 1\).</p>
              <div class="theorem-block">
                <h4>Example&nbsp;2 (Constant step sizes)</h4>
                <p>Take \(\bar{\alpha}_k = \bar{\alpha}\) and \(\delta_i = \delta\). Then \(\beta_k = (1 - \bar{\alpha})^{-k+1}\) and \(\alpha_k = \bar{\alpha} (1 - \bar{\alpha})^{-k}\), leading to</p>
                <p class="math-display">\[U_{k+1} - h(\lambda_{k+1}) \le \tfrac{1}{2} C \big((1 - \bar{\alpha})^{k-1} + \bar{\alpha}\big) + \frac{\delta}{\bar{\alpha}}.\]</p>
                <p>Choosing \(\bar{\alpha} = 1 - (k - 1)^{-1/(k - 2)} = 1 - e^{-\frac{\log(k - 1)}{k - 2}} \in \big[\tfrac{1}{e} \tfrac{\log(k - 1)}{k - 2}, \tfrac{\log(k - 1)}{k - 2}\big]\) yields</p>
                <p class="math-display">\[U_{k+1} - h(\lambda_{k+1}) \le \tfrac{1}{2} C\left(\frac{1}{k - 1} + \frac{\log(k - 1)}{k - 2}\right) + \frac{\delta}{\bar{\alpha}},\]</p>
                <p>so achieving an \(\widetilde{\mathcal{O}}(1/k)\) rate requires \(\delta = \mathcal{O}(\log k / k^2)\). No \(\alpha > 1/2\) guarantees a final Frank-Wolfe gap of order \(\mathcal{O}(\delta)\).</p>
              </div>
            </section>

            <section class="mb-5">
              <h2 id="projection-lmo">5.&nbsp;Projection vs.&nbsp;LMO: accurate linear minimization beats coarse projection</h2>
              <p>Let \((\cdot,\cdot)\) denote the Euclidean inner product with corresponding norm \(\|\cdot\|\). For a nonempty compact convex set \(C\subset\mathbb{R}^d\), define the projection and the linear minimization oracle by</p>
              <p class="math-display">\[\operatorname{Proj}_C(x) = \arg\min_{c\in C} \tfrac{1}{2}\|c - x\|^2, \qquad \operatorname{LMO}_C(z) = \arg\min_{c\in C} (c, z).\]</p>
              <p>An element \(p\in C\) is an \(\varepsilon\)-approximate projection if</p>
              <p class="math-display">\[\tfrac{1}{2}\|p - x\|^2 \le \min_{c\in C} \tfrac{1}{2}\|c - x\|^2 + \varepsilon,\]</p>
              <p>and \(v\in C\) is an \(\varepsilon\)-accurate LMO point if \(0 \le (v,x) - \min_{c\in C} (c,x) \le \varepsilon\).</p>
              <div class="theorem-block">
                <h4>Proposition&nbsp;2</h4>
                <p>If \(p\in \varepsilon\)-\(\operatorname{Proj}_C(x)\), then for all \(c \in C\)</p>
                <p class="math-display">\[(c - p, x - p) \le \varepsilon + \tfrac{1}{2} \|c - p\|^2.\]</p>
                <p><strong>Proof.</strong> From the definition of \(p\), \(\tfrac{1}{2}\|p - x\|^2 \le \tfrac{1}{2}\|c - x\|^2 + \varepsilon\) for all \(c\in C\). Expanding the squares gives \((c - p, x - p) \le \varepsilon + \tfrac{1}{2} \|c - p\|^2\). \(\square\)</p>
              </div>
              <div class="theorem-block">
                <h4>Theorem&nbsp;5 (From \(K\)-projection to accurate LMO)</h4>
                <p>Let \(x\in\mathbb{R}^d\) and \(C\subset\mathbb{R}^d\) be nonempty, compact, and convex. Denote the diameter \(\delta_C = \sup_{c_1,c_2\in C}\|c_1 - c_2\|\) and the radius \(\mu_C = \sup_{c\in C}\|c\|\). Take \(v\in \operatorname{LMO}_C(x)\) and let \(p'\in C\) be a \(K\)-approximate projection of \(-\lambda x\) for some \(\lambda > 0\). Then</p>
                <p class="math-display">\[0 \le (p', x) - (v, x) \le \frac{K + \tfrac{1}{2} \delta_C^2 + \min\{\mu_C \delta_C, \mu_C^2\}}{\lambda}.\]</p>
                <p>Consequently, choosing \(\lambda \ge \big(K + \tfrac{1}{2} \delta_C^2 + \min\{\mu_C \delta_C, \mu_C^2\}\big)/\varepsilon\) guarantees \((p', x) \le \min_{c\in C} (c, x) + \varepsilon\), i.e., \(p'\in \varepsilon\)-\(\operatorname{LMO}_C(x)\).</p>
                <p><strong>Proof.</strong> Proposition&nbsp;2 gives</p>
                <p class="math-display">\[(c - p', -\lambda x - p') \le K + \tfrac{1}{2} \|c - p'\|^2, \qquad \forall c \in C.\]</p>
                <p>Choosing \(c = v\) yields</p>
                <p class="math-display">\[\lambda (p', x) - \lambda (v, x) \le K + \tfrac{1}{2} \|v - p'\|^2 + (p', v - p').\]</p>
                <p>Thus</p>
                <p class="math-display">\begin{aligned}
                  \lambda (p' - v, x) &\le K + \tfrac{1}{2} \|v - p'\|^2 + (p', v - p') \\
                  &= K + \tfrac{1}{2} \|v - p'\|^2 + (v, p') - \|p'\|^2 \\
                  &\le K + \tfrac{1}{2} \|v - p'\|^2 + \|p'\| (\|v\| - \|p'\|) \\
                  &\le K + \tfrac{1}{2} \delta_C^2 + \min\{\mu_C \delta_C, \mu_C^2\}.
                \end{aligned}</p>
                <p>Hence \((p', x) - (v, x) \le [K + \tfrac{1}{2} \delta_C^2 + \min\{\mu_C \delta_C, \mu_C^2\}] / \lambda\). The nonnegativity follows because \(v\) minimizes \((c, x)\) over \(C\). \(\square\)</p>
              </div>
              <p>This result extends Woodstock (2025), which handled exact projections on polyhedral sets, to approximate projections on arbitrary compact convex sets.</p>
            </section>

            <section class="mb-5">
              <h2 id="acknowledgements">Acknowledgements</h2>
              <p>I thank the reviewers for their helpful comments.</p>
            </section>
          </article>
        </div>
      </div>
    </div>
  </main>

  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
</body>
</html>
